{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Apply ETL Processes**:\n",
    "  1. **Extract**: Crawl data from databases (data sources) and save as '.csv' files using Pandas and pyodbc for database connection.\n",
    "  2. **Load**: Load these files into the data lake using Sqoop.\n",
    "  3. **Extract**: Retrieve data from the data lake using Sqoop.\n",
    "  4. **Transform**: Preprocess the data based on requirements and the data warehouse structure using Pandas and potentially Spark.\n",
    "  5. **Load**: Load the ready-to-use data into the data warehouse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc # to connect databases\n",
    "# import numpy as np # maybe needed for data transforming/cleaning \n",
    "from pysqoop.SqoopImport import Sqoop\n",
    "import os # create directory to store data for each database\n",
    "import subprocess # to run sqoop\n",
    "from pyspark.sql import SparkSession # csv to Hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. **Extract**: Crawl data from databases (data sources) and save as '.csv' files using Pandas and pyodbc for database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_tableNames = pd.read_sql(sql_retrieve_table_names, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_17452\\3243945441.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n"
     ]
    }
   ],
   "source": [
    "# connect to the (original) database\n",
    "driver = '{ODBC Driver 17 for SQL Server}'\n",
    "host = 'HOAIBAO'\n",
    "database = 'Wisdom_rubberProject_Practice_3'\n",
    "trusted_connection = 'yes'\n",
    "# Create the connection string\n",
    "connection_string = f\"DRIVER={driver};SERVER={host};DATABASE={database};Trusted_Connection={trusted_connection}\"\n",
    "# because this is a local connection, so I chose to use 'trusted_connection'\n",
    "# in many cases, consider using 'username' and 'password' to connect to the database\n",
    "connection = pyodbc.connect(connection_string)\n",
    "\n",
    "# get the table names in the database\n",
    "sql_retrieve_table_names = \"select name from sys.tables where name <> 'sysdiagrams';\"\n",
    "df_tableNames = pd.read_sql(sql_retrieve_table_names, connection)\n",
    "\n",
    "data_dict = {}\n",
    "for table in df_tableNames['name']:\n",
    "    sql_retrieve_data = f\"select * from [{table}]\" # e.g: [plan]\n",
    "    data_dict[table] = pd.read_sql(sql_retrieve_data, connection)\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: Environment\n",
      "Table: Drone\n",
      "Table: DroneInformation\n",
      "Table: DroneImage\n",
      "Table: ChargingStation\n",
      "Table: ChargingStatus\n",
      "Table: Task\n",
      "Table: Country\n",
      "Table: Region\n",
      "Table: Address\n",
      "Table: Account\n",
      "Table: UserInfo\n",
      "Table: Field\n",
      "Table: RubberTree\n",
      "Table: RubberTreeInformation\n",
      "Table: Plan\n",
      "Table: PlanDetail\n",
      "Table: Lidar\n",
      "Table: Camera\n",
      "Table: Radar\n",
      "Table: SensorControlSystem\n",
      "Table: Robot\n",
      "Table: Energy\n",
      "Table: RobotTapping\n",
      "Table: Blade\n"
     ]
    }
   ],
   "source": [
    "for tableName, data in data_dict.items():\n",
    "    print(\"Table: \" + tableName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'Wisdom_rubberProject_Practice_3' already exists.\n"
     ]
    }
   ],
   "source": [
    "# use 'os' module to create a new directory to store data from the database\n",
    "try:\n",
    "    # Create the directory\n",
    "    os.mkdir(f\"{database}\")\n",
    "    print(f\"Directory '{database}' created successfully.\")\n",
    "    for tableName, data in data_dict.items():\n",
    "        # print(type(data))\n",
    "        data.to_csv(f\"{database}/{tableName}.csv\", index=False)\n",
    "except FileExistsError:\n",
    "    print(f\"Directory '{database}' already exists.\")\n",
    "\n",
    "# after successfully run this code, a new directory '{database}/' should be appear.\n",
    "# This directory contains all the data from the database as '.csv' format, can be useful for backup data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Load**: Load these files into the data lake using Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wisdom_rubberProject_Practice_3/Account.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Account\n",
      "uploaded Account.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Account\n",
      "Wisdom_rubberProject_Practice_3/Address.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Address\n",
      "uploaded Address.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Address\n",
      "Wisdom_rubberProject_Practice_3/Blade.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Blade\n",
      "uploaded Blade.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Blade\n",
      "Wisdom_rubberProject_Practice_3/Camera.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Camera\n",
      "uploaded Camera.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Camera\n",
      "Wisdom_rubberProject_Practice_3/ChargingStation.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/ChargingStation\n",
      "uploaded ChargingStation.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/ChargingStation\n",
      "Wisdom_rubberProject_Practice_3/ChargingStatus.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/ChargingStatus\n",
      "uploaded ChargingStatus.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/ChargingStatus\n",
      "Wisdom_rubberProject_Practice_3/Country.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Country\n",
      "uploaded Country.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Country\n",
      "Wisdom_rubberProject_Practice_3/Drone.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Drone\n",
      "uploaded Drone.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Drone\n",
      "Wisdom_rubberProject_Practice_3/DroneImage.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/DroneImage\n",
      "uploaded DroneImage.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/DroneImage\n",
      "Wisdom_rubberProject_Practice_3/DroneInformation.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/DroneInformation\n",
      "uploaded DroneInformation.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/DroneInformation\n",
      "Wisdom_rubberProject_Practice_3/Energy.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Energy\n",
      "uploaded Energy.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Energy\n",
      "Wisdom_rubberProject_Practice_3/Environment.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Environment\n",
      "uploaded Environment.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Environment\n",
      "Wisdom_rubberProject_Practice_3/Field.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Field\n",
      "uploaded Field.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Field\n",
      "Wisdom_rubberProject_Practice_3/Lidar.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Lidar\n",
      "uploaded Lidar.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Lidar\n",
      "Wisdom_rubberProject_Practice_3/Plan.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Plan\n",
      "uploaded Plan.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Plan\n",
      "Wisdom_rubberProject_Practice_3/PlanDetail.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/PlanDetail\n",
      "uploaded PlanDetail.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/PlanDetail\n",
      "Wisdom_rubberProject_Practice_3/Radar.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Radar\n",
      "uploaded Radar.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Radar\n",
      "Wisdom_rubberProject_Practice_3/Region.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Region\n",
      "uploaded Region.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Region\n",
      "Wisdom_rubberProject_Practice_3/Robot.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Robot\n",
      "uploaded Robot.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Robot\n",
      "Wisdom_rubberProject_Practice_3/RobotTapping.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/RobotTapping\n",
      "uploaded RobotTapping.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/RobotTapping\n",
      "Wisdom_rubberProject_Practice_3/RubberTree.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/RubberTree\n",
      "uploaded RubberTree.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/RubberTree\n",
      "Wisdom_rubberProject_Practice_3/RubberTreeInformation.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/RubberTreeInformation\n",
      "uploaded RubberTreeInformation.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/RubberTreeInformation\n",
      "Wisdom_rubberProject_Practice_3/SensorControlSystem.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/SensorControlSystem\n",
      "uploaded SensorControlSystem.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/SensorControlSystem\n",
      "Wisdom_rubberProject_Practice_3/Task.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Task\n",
      "uploaded Task.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Task\n",
      "Wisdom_rubberProject_Practice_3/UserInfo.csv\n",
      "Writing to HDFS path: hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/UserInfo\n",
      "uploaded UserInfo.csv to hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/UserInfo\n",
      "Upload completed.\n",
      "Successfully uploaded files to HDFS.\n"
     ]
    }
   ],
   "source": [
    "# NOTICE:\n",
    "\n",
    "# Make sure that the HDFS is running\n",
    "# First, open Command Prompt at the directory \"\\hadoop-3.3.0\\sbin\"\n",
    "# Run as admminstrator\n",
    "# Then use the command line \"start-all.cmd\" to start HDFS\n",
    "\n",
    "# Create a 'user' directory in the Hadoop folder if it doesn't exist\n",
    "# Run the command \"hdfs dfs -mkdir -p /user/<your_username>/<data_contained_folder>\"\n",
    "# In this case, using directory \"/user/hoaibao/data\"\n",
    "# In this code, I already run a 'os' function to create that directory\n",
    "\n",
    "\n",
    "# Define local and HDFS directories\n",
    "spark_local = SparkSession.builder.master('local[1]').appName(\"local_CSV_to_HDFS\").getOrCreate()\n",
    "local_directory = f'{database}/'\n",
    "hdfs_directory  = f'hdfs://localhost:9001/user/hoaibao/data/{database}' # chech the 'yarn-site.xml' file to see the port, in this case, I setup to 9001, default usually is 9000\n",
    "\n",
    "# Create the target directory in HDFS if it doesn't exist\n",
    "os.system(f\"hdfs dfs -mkdir -p {hdfs_directory}\")\n",
    "\n",
    "csv_files = [f for f in os.listdir(local_directory) if f.endswith('.csv')]\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    local_path = os.path.join(local_directory, csv_file)\n",
    "    print(local_path)\n",
    "    # read csv file to dataframe using Spark\n",
    "    df = spark_local.read.option('header', True).format('csv').load(local_path) #  use the inferSchema option to automatically infer the data types of the columns in your CSV file\n",
    "    # Define HDFS path\n",
    "    # hdfs_path = os.path.join(hdfs_directory, os.path.splitext(csv_file)[0])\n",
    "    # print(hdfs_path)\n",
    "    hdfs_path = f\"{hdfs_directory}/{os.path.splitext(csv_file)[0]}\"\n",
    "    print(f\"Writing to HDFS path: {hdfs_path}\")\n",
    "    \n",
    "    # write dataframe to HDFS\n",
    "    # df.coalesce(1).write.mode('overwrite').option('header','true').csv('hdfs://path/df.csv')\n",
    "    df.coalesce(1).write.mode('overwrite').option('header', True).csv(hdfs_path) # \"overwrite\" option: always replace existed data in the hdfs (if there is) with the latest data\n",
    "    # alternative modes: 'append', 'ignore', 'error'\n",
    "    \n",
    "    print(f'uploaded {csv_file} to {hdfs_path}')\n",
    "\n",
    "spark_local.stop()\n",
    "        \n",
    "print('Upload completed.')\n",
    "print('Successfully uploaded files to HDFS.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_local.stop()\n",
    "# sometime the spark session is not stopped due to the malfunction of the code, check it carefully\n",
    "\n",
    "# After successfully write data into the HDFS, we can verify the data by using this command : \"hdfs dfs -ls hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3\"\n",
    "# It should print out a list of contents inside the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **Extract**: Retrieve data from the data lake using Sqoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_files = subprocess.check_output(['hdfs', 'dfs', '-ls', hdfs_directory], shell=True).decode('utf-8').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Found 25 items\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Account\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Address\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Blade\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Camera\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/ChargingStation\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/ChargingStatus\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Country\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Drone\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/DroneImage\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/DroneInformation\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Energy\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Environment\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Field\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Lidar\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Plan\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/PlanDetail\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Radar\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Region\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Robot\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/RobotTapping\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/RubberTree\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/RubberTreeInformation\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/SensorControlSystem\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Task\\r',\n",
       " 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/UserInfo\\r',\n",
       " '']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Found 25 items\\r'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories found: ['hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Account', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Address', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Blade', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Camera', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/ChargingStation', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/ChargingStatus', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Country', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Drone', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/DroneImage', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/DroneInformation', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Energy', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Environment', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Field', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Lidar', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Plan', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/PlanDetail', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Radar', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Region', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Robot', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/RobotTapping', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/RubberTree', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/RubberTreeInformation', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/SensorControlSystem', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Task', 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/UserInfo']\n"
     ]
    }
   ],
   "source": [
    "# Extract directory paths\n",
    "directories = [line.split()[-1] for line in hdfs_files if line and line.startswith('drwx')]\n",
    "print(\"Directories found:\", directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Account',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Address',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Blade',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Camera',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/ChargingStation',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/ChargingStatus',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Country',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Drone',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/DroneImage',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/DroneInformation',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Energy',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Environment',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Field',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Lidar',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Plan',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/PlanDetail',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Radar',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Region',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Robot',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/RobotTapping',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/RubberTree',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/RubberTreeInformation',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/SensorControlSystem',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Task',\n",
       " 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/UserInfo']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-----+\n",
      "|      _c0|     _c1|      _c2|  _c3|\n",
      "+---------+--------+---------+-----+\n",
      "|AccountID|Username| Password| Role|\n",
      "|        1|   user1|password1|Admin|\n",
      "|        2|   user2|password2| User|\n",
      "|        3|   user3|password3| User|\n",
      "|        4|   user4|password4| User|\n",
      "|        5|   user5|password5| User|\n",
      "+---------+--------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_local = SparkSession.builder.master('local[1]').appName('readHDFS').getOrCreate()\n",
    "\n",
    "df = spark_local.read.csv('hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Account/part-00000-4d5ca4a3-29ac-4c15-a5f9-64629d8a0697-c000.csv')\n",
    "df.show()\n",
    "spark_local.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start spark session\n",
      "Account part-00000-4d5ca4a3-29ac-4c15-a5f9-64629d8a0697-c000.csv\n",
      "Address part-00000-c4d2362c-a356-4ee2-8321-57b949aabe3d-c000.csv\n",
      "Blade part-00000-2f6e9574-3d5c-468c-ae20-a27179088604-c000.csv\n",
      "Camera part-00000-2663e7c3-ad22-4f2a-9218-b9181e279b38-c000.csv\n",
      "ChargingStation part-00000-210704ea-0600-4fc8-9187-45e457032179-c000.csv\n",
      "ChargingStatus part-00000-56644563-ab44-43d0-81d8-65fc06882898-c000.csv\n",
      "Country part-00000-ea0b38e5-f487-40e2-9778-a815d46fe58b-c000.csv\n",
      "Drone part-00000-7e1c8286-31ef-4af8-b62d-2c14e2873733-c000.csv\n",
      "DroneImage part-00000-4a3d47ba-72b7-41dc-8eae-e2182c002f5a-c000.csv\n",
      "DroneInformation part-00000-d0c0e460-d9e0-4346-8e40-d5f849e93518-c000.csv\n",
      "Energy part-00000-d53a0855-bada-4d0f-9691-5f78922095fb-c000.csv\n",
      "Environment part-00000-17f87e2b-76f1-4f79-8cb8-ee56464d64e5-c000.csv\n",
      "Field part-00000-8514c4d3-f9c8-4cc9-9d62-721c028c1afa-c000.csv\n",
      "Lidar part-00000-1921fb6c-4d7d-4469-a7ca-09f0ddaabb77-c000.csv\n",
      "Plan part-00000-00f60b46-86ac-40b1-a65a-741d56f91f52-c000.csv\n",
      "PlanDetail part-00000-e746e67e-d017-4c10-9712-39f8b6259183-c000.csv\n",
      "Radar part-00000-0d707771-0a42-4e8d-b00d-b0ad18bf47f2-c000.csv\n",
      "Region part-00000-34a86102-c051-4da7-a53e-808750614237-c000.csv\n",
      "Robot part-00000-dabf390d-1590-4a51-8718-874c79301568-c000.csv\n",
      "RobotTapping part-00000-fcab9165-ef17-4b32-8b3c-5fb95015ff9e-c000.csv\n",
      "RubberTree part-00000-7e10610d-3b9e-4336-ab5d-89eb6c5a2fe4-c000.csv\n",
      "RubberTreeInformation part-00000-f681d184-8281-48e4-ba3b-9c5e2ea06185-c000.csv\n",
      "SensorControlSystem part-00000-e220d844-4d03-4117-b19f-4d03031ecdf5-c000.csv\n",
      "Task part-00000-b0c0a0a7-e966-49ea-a479-efeded2eed05-c000.csv\n",
      "UserInfo part-00000-68fc34d9-f1d2-4cdc-bfdf-06aca5179174-c000.csv\n",
      "stop spark session\n"
     ]
    }
   ],
   "source": [
    "hdfs_data = subprocess.check_output(['hdfs', 'dfs', '-ls', hdfs_directory], shell=True).decode('utf-8').split('\\n')\n",
    "# e.g -> 'drwxr-xr-x   - tranh supergroup          0 2024-07-23 12:56 hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Account\\r'\n",
    "hdfs_directories = [line.split()[-1] for line in hdfs_data if line.startswith('drwxr')]\n",
    "# e.g -> 'hdfs://localhost:9001/user/hoaibao/data/Wisdom_rubberProject_Practice_3/Account'\n",
    "spark_local = SparkSession.builder.master('local[1]').appName('readHDFS').getOrCreate()\n",
    "print('start spark session')\n",
    "\n",
    "data_dict = {}\n",
    "for directory in hdfs_directories:\n",
    "    contentFile = subprocess.check_output(f'hdfs dfs -ls {directory}', shell=True)\n",
    "    # e.g: b'Found 2 items\\r\\n-rw-r--r--   3 tranh supergroup          0 2024-07-23 12:56 /user/hoaibao/data/Wisdom_rubberProject_Practice_3/Account/_SUCCESS\\r\\n-rw-r--r--   3 tranh supergroup        155 2024-07-23 12:56 /user/hoaibao/data/Wisdom_rubberProject_Practice_3/Account/part-00000-4d5ca4a3-29ac-4c15-a5f9-64629d8a0697-c000.csv\\r\\n'\n",
    "    table_name = contentFile.split()[-1].split(b'/')[-2].decode('utf-8') # e.g: \"Account\"\n",
    "    file_name  = contentFile.split()[-1].split(b'/')[-1].decode('utf-8') # e.g: \"part-00000-4d5ca4a3-29ac-4c15-a5f9-64629d8a0697-c000.csv\"\n",
    "    print(table_name, file_name)\n",
    "    data_dict[table_name] = spark_local.read.option('header', True).csv(f'{hdfs_directory}/{table_name}/{file_name}')\n",
    "    \n",
    "    # Next:\n",
    "    # Option (1): Direct Import from Data Lake to SQL Data Warehouse\n",
    "    # Perform data TRANSFORMATIONS AND LOAD DIRECTLY into the data warehouse WITHIN THIS LOOP (SPARK SESSION)\n",
    "    #    - Advantages:\n",
    "    #        * Immediate processing and loading without intermediate steps\n",
    "    #        * Efficient for straightforward transformations\n",
    "    #        * Saves storage space as intermediate files are not created\n",
    "    #    - Disadvantages:\n",
    "    #        * Less flexible for complex transformations\n",
    "    #        * Can be challenging to debug if transformations fail\n",
    "    #        * Limited to data sizes that can be handled in memory during a single session\n",
    "    \n",
    "    # Option (2): Staging Data Locally\n",
    "    # SAVE DATA LOCALLY FIRST and perform transformations/loads later\n",
    "    #    - Advantages:\n",
    "    #        * Allows for more complex transformations to be applied\n",
    "    #        * Easier to debug transformation steps individually\n",
    "    #        * Suitable for very large datasets that may not fit into memory at once\n",
    "    #        * Provides the ability to re-process data without re-reading from HDFS\n",
    "    #    - Disadvantages:\n",
    "    #        * Requires additional storage space for intermediate files\n",
    "    #        * Involves additional I/O operations, which can slow down the process\n",
    "    #        * May involve more steps in the workflow, making it more complex to manage\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # Choose one of the options based on the above considerations\n",
    "    # Example for Option 1:\n",
    "    # transformed_df = data_dict[table_name].withColumnRenamed(\"_c0\", \"Column1\").withColumnRenamed(\"_c1\", \"Column2\")\n",
    "    # write_to_data_warehouse(transformed_df, table_name)\n",
    "    \n",
    "    # Example for Option 2:\n",
    "    # local_path = f'/local/path/{table_name}.csv'\n",
    "    # data_dict[table_name].write.option(\"header\", True).csv(local_path)\n",
    "\n",
    "    # \n",
    "    \n",
    "spark_local.stop()\n",
    "print('stop spark session')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['Account']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. **Transform**: Preprocess the data based on requirements and the data warehouse structure using Pandas and potentially Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. **Load**: Load the ready-to-use data into the data warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
